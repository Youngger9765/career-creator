# CLAUDE.md - Project Guidelines

---

## ğŸš¨ AT THE START OF EVERY SESSION (CRITICAL)

**WHEN YOU START A NEW CONVERSATION, YOU MUST:**

1. **Read this CLAUDE.md file**

2. **Print the following rules to remind yourself**:

   ```text
   ğŸ“‹ SESSION RULES CHECKLIST:

   âœ… BEFORE ANY COMMIT:
      - User must test the feature first
      - NEVER auto-commit without user approval
      - Ask: "Have you tested this? Ready to commit?"

   âœ… AFTER EVERY `git push`:
      - DO NOT STOP and wait for user
      - IMMEDIATELY run: gh run list --branch <branch> --limit 1
      - IMMEDIATELY run: gh run watch <run-id>
      - If fails: read logs, fix, push again
      - If succeeds: get URL, run Playwright tests
      - NEVER say "pushed successfully" and move on

   âœ… DEPLOYMENT VERIFICATION:
      - Always get fresh URL from logs (URLs change!)
      - Test actual deployed feature with Playwright
      - Report full results to user

   âŒ NEVER:
      - Push without user testing
      - Push and forget
      - Use cached/old URLs
      - Skip deployment verification
   ```

3. **Then proceed with the user's request**

---

## âš ï¸ CI/CD Deployment Protocol (CRITICAL)

**AFTER EVERY `git push`, YOU MUST:**

1. **Monitor deployment immediately**:

   ```bash
   gh run list --branch <branch> --limit 1
   gh run watch <run-id>
   ```

2. **Verify deployment success**:

   ```bash
   gh run list --branch <branch> --limit 1 --json status,conclusion
   ```

3. **Test on actual staging URL** (not hardcoded old URLs):

   ```bash
   # Get actual URL from deployment logs
   gh run view <run-id> --log | grep "Service URL:"

   # Run Playwright tests against staging
   npx playwright test <test-file> --project=chromium
   ```

4. **If deployment fails**:
   - Read the error logs: `gh run view <run-id> --log`
   - Fix the issue
   - Never leave broken deployments

**DO NOT:**

- âŒ Push and forget
- âŒ Assume CI/CD success means the feature works
- âŒ Use old/cached URLs for testing
- âŒ Skip verification tests

**REMEMBER:**

- Staging URL changes with each service redeployment
- Always get fresh URL from deployment logs
- Test the actual deployed feature, not just build success

---

## Project Overview

Building an online card consultation system for career counselors and their visitors.

## Commit Message Convention

- **Language**: Always use English for commit messages
- **Format**: Use conventional commits format
  - `feat:` for new features
  - `fix:` for bug fixes
  - `docs:` for documentation
  - `style:` for formatting changes
  - `refactor:` for code restructuring
  - `test:` for tests
  - `chore:` for maintenance tasks

## Code Standards

### Frontend

- **Framework**: Next.js 14 with TypeScript
- **Styling**: Tailwind CSS + shadcn/ui
- **State Management**: Zustand
- **Drag and Drop**: @dnd-kit
- **API Client**: Axios + React Query

### Backend

- **Framework**: FastAPI with Python 3.11+
- **ORM**: SQLModel (unified SQLAlchemy + Pydantic)
- **Database Migrations**: Alembic
- **Testing**: pytest
- **Async**: asyncio

### Database & Infrastructure

- **Database**: PostgreSQL (Supabase)
- **Deployment**: GCP Cloud Run
- **Storage**: GCP Cloud Storage

## Development Guidelines

1. **Mobile First**: Always prioritize mobile experience
2. **Type Safety**: Use TypeScript for all code
3. **Component Structure**: Keep components small and focused
4. **Testing**: Write tests for critical functionality
5. **Documentation**: Comment complex logic
6. **âš ï¸ NO AUTO COMMIT**: NEVER commit new features without user testing
   first. Always wait for user to test and approve before committing.
7. **âš ï¸ NEVER USE --no-verify**: Always fix TypeScript and linting errors
   properly. Pre-commit hooks exist to maintain code quality. Fix the errors,
   don't bypass them!

## TDD with AI Development (Kent Beck's Principles)

### Canon TDD Process

Following Kent Beck's canonical TDD workflow:

1. **List Expected Behaviors**: Before coding, list all expected variants in
   the new behavior
   - Basic case
   - Edge cases (timeouts, missing data, etc.)
   - Existing behavior that shouldn't break

2. **Red-Green-Refactor Cycle**:
   - **Red**: Write one failing test
   - **Green**: Make it pass (even with ugly code)
   - **Refactor**: Clean up when test is green

3. **One Test at a Time**: Focus on a single test, make it pass, then
   move to the next

### TDD as AI "Superpower"

According to Kent Beck, TDD is a "superpower" when working with AI agents:

1. **Tests as Prompts**: Writing tests first essentially "prompts" the AI
   with exact requirements
2. **Guard Rails**: Tests prevent AI from introducing regressions
3. **Small Context Window**: Keep tests focused to maintain high code quality
4. **Immediate Feedback**: Tests catch when AI strays off course

### Common Mistakes to Avoid

- Don't write tests without assertions just for coverage
- Don't convert all test items to concrete tests at once
- Don't mix refactoring with making tests pass
- Watch for AI trying to delete/disable tests to "pass"

### AI-Assisted TDD Workflow

1. Write a failing test that describes the desired behavior
2. Use AI to generate code that passes the test
3. Feed test results back to AI for iterations
4. Refactor with confidence knowing tests protect you
5. Repeat for next behavior

### Key Benefits

- **Better Context**: Tests provide crucial context to AI assistants
- **Higher Quality**: Edge cases in tests lead to more robust generated code
- **Faster Iteration**: Clear requirements reduce back-and-forth
- **Confidence**: Tests catch AI-introduced bugs immediately

## File Structure (Monorepo)

```text
/frontend
  /src
    /app          # Next.js app router pages
    /components   # Reusable components
    /lib          # Utilities and helpers
    /hooks        # Custom React hooks
    /types        # TypeScript type definitions
    /styles       # Global styles

/backend
  /app
    /api          # API endpoints (FastAPI routers)
    /models       # SQLModel models (unified ORM + schemas)
    /core         # Core configs, database, roles
    /services     # Business logic (future)
  /tests          # pytest tests (TDD approach)
  /alembic        # Database migrations
```

## Key Decisions

- Use polling for MVP (no WebSocket initially)
- Visitors don't need registration
- Room expires after 7 days by default
- Implement all 3 game types (è·èƒ½ç›¤é»å¡ã€åƒ¹å€¼å°èˆªå¡ã€è·æ¸¸æ—…äººå¡) with
  universal rules engine

## Game Architecture

For detailed game architecture and implementation, see [GAME_DESIGN.md](./GAME_DESIGN.md)

## E2E Testing with Playwright

**NO EXCUSES. é‡åˆ°å•é¡Œç›´æ¥è§£æ±ºï¼Œä¸è¦æ‰¾è—‰å£ã€‚**

### å¿ƒæ³•

1. **å…ˆè®€çµ„ä»¶** - çŒœæ¸¬é¸æ“‡å™¨ = æµªè²»æ™‚é–“
2. **æ¸¬å®Œæ•´æµç¨‹** - ä¸è¦åªæ¸¬é é¢èƒ½ä¸èƒ½é–‹
3. **éƒ¨ç½²å¾Œå¿…æ¸¬** - CI/CD éäº†ä¸ä»£è¡¨åŠŸèƒ½å°

### æ¨™æº–æ­¥é©Ÿ

```bash
# 1. æ‰¾åˆ°çµ„ä»¶ï¼Œçœ‹çœŸå¯¦çš„é¸æ“‡å™¨
cat frontend/src/app/login/page.tsx

# 2. å¯«æ¸¬è©¦ (ç”¨çµ„ä»¶è£¡çš„ ID/class)
# frontend/e2e/feature.spec.ts

# 3. ç›£æ§éƒ¨ç½²
gh run watch <run-id>

# 4. è·‘æ¸¬è©¦é©—è­‰
npx playwright test --project=webkit
```

### é‡åˆ°å•é¡Œæ€éº¼è¾¦

**âŒ éŒ¯èª¤åšæ³•**:

- "å¯èƒ½æ²’æœ‰é€™å€‹å¸³è™Ÿ" â†’ ç°¡åŒ–æ¸¬è©¦
- "é¸æ“‡å™¨å¯èƒ½ä¸å°" â†’ åªæ¸¬åŸºç¤åŠŸèƒ½
- "ç’°å¢ƒå¯èƒ½æœ‰å•é¡Œ" â†’ è·³éé©—è­‰

**âœ… æ­£ç¢ºåšæ³•**:

1. è®€ç™»å…¥é é¢çµ„ä»¶ â†’ æ‰¾åˆ° `input#email`, `input#password`
2. çœ‹æ¸¬è©¦å¸³è™Ÿåˆ—è¡¨ â†’ ç”¨ `demo.counselor@example.com`
3. å¯«å®Œæ•´æ¸¬è©¦ â†’ ç™»å…¥ â†’ é€²æˆ¿é–“ â†’ é©—è­‰åŠŸèƒ½
4. æ¸¬è©¦å¤±æ•— â†’ çœ‹æˆªåœ– â†’ ä¿®æ­£é¸æ“‡å™¨ â†’ é‡è·‘

**æ ¸å¿ƒ**: ä½ æœ‰æ‰€æœ‰å·¥å…·å’Œè³‡è¨Šï¼Œç›´æ¥è§£æ±ºå•é¡Œã€‚

### Backend Testing (pytest)

```bash
cd backend
pytest                          # Run all tests
pytest -v                       # Verbose output
pytest --cov                    # With coverage
alembic upgrade head             # Run migrations
```

### Frontend Commands

```bash
cd frontend
npm run dev      # Development server (port 3000)
npm run build    # Production build
npm run lint     # Lint check
npx playwright test              # Run E2E tests
npx playwright test --ui         # Run with UI mode
npx playwright codegen <url>     # Generate test code
```

### Full Stack

```bash
docker-compose up               # Run both frontend and backend
```

## Deployment

- Platform: GCP Cloud Run
- Database: Supabase PostgreSQL
- Storage: GCP Cloud Storage
- Environment: Production branch = main

### GCloud Configuration

Project uses dedicated gcloud configuration:

```bash
# Switch to career-creator configuration
gcloud config configurations activate career-creator

# Verify configuration
gcloud config list
```

**Expected configuration:**

- **Active configuration**: `career-creator`
- **Project**: `career-creator-card`
- **Region**: `asia-east1`

### Cloud Run Services

**âš ï¸ WARNING: URLs may change after service redeployment.**
**Always get fresh URL from deployment logs.**

To get current staging URL:

```bash
gh run list --branch staging --limit 1
gh run view <run-id> --log | grep "Service URL:"
```

Latest known URLs (verify before use):

- **Staging**: `https://career-creator-frontend-staging-849078733818.asia-east1.run.app`
- **Production**: `https://career-creator-frontend-production-990202338378.asia-east1.run.app`

### GitHub Actions Deployment

Auto-deployment configured:

- `staging` branch â†’ staging environment
- `main` branch â†’ production environment

Service Account configured for automated deployment.

---

## Last Updated

2025-10-22
